{
  "hash": "c2adb9deb77b74bff7025adb677eb0e3",
  "result": {
    "markdown": "---\ntitle: \"Psychology of Scientific Fraud Data Analysis Plan\"\nauthor: Benjamin Zubaly\ndate: November 19, 2023\nformat: \n  html:\n    self-contained: true\n    toc: true\nknitr:\n  opts_chunk: \n      warning: false\n      message: false\neditor: visual\nbibliography: references.bib\ncsl: apa.csl\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(eval = FALSE)\n```\n:::\n\n\n# Preparation for Data Analysis\n\nThe following subsections will detail the processes of (1) data collection, (2) data entry, (3) data preprocessing, and (4) text analysis procedure as relatively linearly to allow this document to guide a linear workflow during execution of the project. Some procedures may not fit neatly within the three subsections neatly while maintaining this workflow-oriented documentation, and some procedures may not be named in their exact procedural chronology because they are more usefully categorized within these three subsections.\n\n## Data Collection\n\n1.  Single-author fraudulent papers (SAFP) will be identified from the Retraction Watch database by sorting by the author and \"reason\" column.\n    -   Fraudulent papers will be defined as any paper marked with a \"data fabrication\" or \"data falsification\" label in the reason column.\n    -   A new csv file (hereafter \"study dataset\") will be created with these papers only and all of the data from the Retraction Watch dataset.\n2.  Multi-author fraudulent papers (MAFP) will then be identified from the retraction watch database and matched according to our criteria (see methodology doc) with SAFPs by sorting the retraction watch database along journal and year. Individual papers will then be investigated through internet searches to attempt matching by other criteria.\n    -   All data from the Retraction Watch database will be retained when adding MAFP to study dataset.\n3.  Once all fraudulent papers have been assembled, online searches for matched papers will be conducted to attain papers with matched characteristics according to our criteria (see methodology doc).\n    -   Genuine papers will be matched with fraudulent papers within author number groups (e.g., single fraudulent matched with single genuine).\n        -   This will ensure matching across all groups where between-group statistical comparisons will be conducted.\n4.  During the collection of all paper types, PDFs of the papers will be saved into separate files on the primary author's computer and identified with their DOI.\n    -   References will be counted from these original PDFs and entered into the study data. This will avoid miscounts due to counting references after potential alterations to references are made during data preprocessing procedures.\n    -   Number of authors will also be counted from these original PDFs and entered into the study data. This will avoid miscounts due to counting authors after potential alterations to this text are made during data preprocessing procedures.\n\n## Data Entry\n\n1.  As mentioned above, data from the retraction watch dataset will be retained for our study data. This data will be done for all fraudulent papers and whenever possible for genuine papers as well.\n2.  During the search for and collection of all paper types, data regarding the matching criteria will be recorded and entered by hand.\n3.  During the search for and collection of all multi-author fraudulent papers, data regarding whether the fraudulent author is the corresponding author will be collected and entered into the study data as a dichotomous variable.\n    -   The ground truth for whether an author is the fraudulent author will be determined by investigating retraction watch article linked in the retraction watch database.\n        -   When making judgments as to attributions of fraud in the articles, we will make the preliminary judgment and study data entry, and he include the parts of the articles that he believes make this decision justified. If there is not enough information to make a judgment, the case will be marked as unknown.\n        -   Then, a second judge will examine these sentences, and in any case of uncertainty the original article will be discussed by the two judges.\n        -   If there remains a dispute, the case will be noted as unknown.\n\n## **Data Preprocessing:**\n\nData preprocessing procedures will utilize Python [@python2023]. All Python scripts will be executed in [Jupyter Notebook](https://jupyter.org \"Link to Jupyter Website Home Page\") [@jupyter2023].\n\n-   All PDFs will be automatically converted to txt file formats using the following Python script[^1] (based on `fitz`).\n\n\n    ::: {.cell python.reticulate='false'}\n    \n    ```{.python .cell-code}\n    import fitz  # PyMuPDF\n    import os\n    \n    # To define the directory containing the PDFs for text extraction\n    pdfs_dir = '/Users/benjaminzubaly/Downloads/PDFs4Extraction'\n    \n    # To define the directory for the output text files\n    output_dir = '/Users/benjaminzubaly/Downloads/pdftotxt'\n    \n    # To create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # To loop through each file in the PDFs directory\n    for filename in os.listdir(pdfs_dir):\n        # To check if the file is a PDF\n        if filename.lower().endswith('.pdf'):  # Lower() is used to handle case-insensitive extensions\n            # Full path to the current PDF file\n            pdf_path = os.path.join(pdfs_dir, filename)\n    \n            # To extract the base name of the PDF file (without the extension)\n            base_name, _ = os.path.splitext(os.path.basename(pdf_path))\n    \n            # To name the new txt file with 'plain_text_' at begininning\n            txt_file_name = os.path.join(output_dir, f'plain_text_{base_name}.txt')\n    \n            # To open the PDF file\n            doc = fitz.open(pdf_path)\n    \n            # To open the txt file for writing\n            with open(txt_file_name, 'w') as txt_file:\n                # To iterate through each page and extract text\n                for page_num in range(len(doc)):\n                    page = doc.load_page(page_num)  # Load the page\n                    text = page.get_text()  # Extract text from the page\n                    txt_file.write(text)  # Write the text to the txt file\n    \n            # To close the document\n            doc.close()\n    ```\n    :::\n\n\n-   These text files will then each be opened manually to remove abstracts, tables, figures, etc. Only main body text will be left in the text files.\n\n    -   Full texts of abstracts will be retained and entered into the study data set via copy and paste.\n\n-   The text will then be cleaned using the following Python script.\n\n    -   The script removes periods that do not end sentences; converts all characters to lowercase; removes all forms of the words retract, withdrawn, introduction, method, procedure, and discussion; removes all relatively common special characters in scientific writing; removes footnotes; removes numbers; replaces newline characters with a space; and removes dates.\n\n    -   This is necessary for reliable text analysis metrics, particularly readability statistics, which heavily rely on words per sentence (and require the removal of special characters such as non-sentence-ending periods).\n\n\n        ::: {.cell python.reticulate='false'}\n        \n        ```{.python .cell-code}\n        import os\n        import re\n        \n        def clean_text(text):\n            # List of common abbreviations with periods\n            abbreviations = [\n                \"Dr\\.\", \"Mr\\.\", \"Mrs\\.\", \"Ms\\.\", \"Jr\\.\", \"Sr\\.\", \"Co\\.\", \"Inc\\.\", \"Ltd\\.\", \"Corp\\.\", \"Prof\\.\",\n                \"Asst\\.\", \"Assoc\\.\", \"Capt\\.\", \"Sgt\\.\", \"Lt\\.\", \"Gen\\.\", \"Pvt\\.\", \"Cpl\\.\", \"Maj\\.\", \"Gov\\.\",\n                \"Pres\\.\", \"V\\.P\\.\", \"Sec\\.\", \"Treas\\.\", \"Mgr\\.\", \"Dir\\.\", \"Dep\\.\", \"Rep\\.\", \"Sen\\.\", \"Amb\\.\",\n                \"Ch\\.\", \"Art\\.\", \"Sect\\.\", \"Est\\.\", \"Dept\\.\", \"Ave\\.\", \"Blvd\\.\", \"St\\.\", \"Rd\\.\", \"Ln\\.\",\n                \"Ct\\.\", \"Sq\\.\", \"Hts\\.\", \"Mtn\\.\", \"Pkwy\\.\", \"Cir\\.\", \"Pl\\.\", \"Bldg\\.\", \"Fl\\.\", \"Apt\\.\",\n                \"Ste\\.\", \"P\\.O\\.\", \"U\\.S\\.\", \"U\\.K\\.\", \"E\\.U\\.\", \"U\\.N\\.\"\n            ]\n            abbreviations_pattern = r'\\b(?:' + '|'.join(abbreviations) + r')'\n            text = re.sub(abbreviations_pattern, lambda x: x.group().replace('.', ''), text, flags=re.IGNORECASE)\n        \n            # Remove periods not at the end of sentences or document\n            text = re.sub(r'\\.(?=(?!\\s[A-Z])|$)', '', text)\n            # Convert to lowercase\n            text = text.lower()\n            # Remove 'retract' and its derivatives in all forms\n            text = re.sub(r'\\bretract(ing|ion|s|ed)?\\b', '', text, flags=re.IGNORECASE)\n            # Remove forms of \"withdraw\" or \"withdrawn\"\n            text = re.sub(r'\\bwithdraw(n|s|ing)?\\b', '', text, flags=re.IGNORECASE)\n            # Remove a wide range of special characters, Greek letters, degree symbol, and superscript numbers 0-10\n            special_chars = r'[@#$%^&*\\(\\)_\\-+=\\{\\}\\[\\]\\|\\\\/:;\"\\'<>,~€£¥¢©®™±²³µπ÷×√∞∑∆≈≠≤≥∂∫∝∩∪∈∉∅∀∃∴∵∼≡↔αβγδεζηθικλμνξοπρστυφχψωΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩ∇⊥°₀₁₂₃₄₅₆₇₈₉₁₀]'\n            text = re.sub(special_chars, '', text)\n            # Replace newline characters with space\n            text = text.replace('\\n', ' ')\n            # Remove all numbers (even within or directly next to letters)\n            text = re.sub(r'\\d+', '', text)\n            # Remove common date formats (basic pattern)\n            text = re.sub(r'\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{1,2},\\s+\\d{4}\\b', '', text)\n            # Remove specified words in all forms\n            text = re.sub(r'\\b(introduction|methods|procedure|discussion)s?\\b', '', text, flags=re.IGNORECASE)\n            # Additional cleaning rules can be added here\n            return text\n        \n        def process_files(folder_path):\n            # Create a new folder for cleaned files\n            clean_folder_path = '/Users/benjaminzubaly/Downloads/clean_txt'\n            if not os.path.exists(clean_folder_path):\n                os.makedirs(clean_folder_path)\n        \n            for filename in os.listdir(folder_path):\n                if filename.endswith('.txt'):\n                    file_path = os.path.join(folder_path, filename)\n                    with open(file_path, 'r') as file:\n                        content = file.read()\n        \n                    cleaned_content = clean_text(content)\n        \n                    # Rename the file with 'clean_' prefix and save in the new folder\n                    new_filename = 'clean_' + filename.replace('prepared_', '')\n                    new_file_path = os.path.join(clean_folder_path, new_filename)\n        \n                    with open(new_file_path, 'w') as file:\n                        file.write(cleaned_content)\n        \n        # Path to the folder\n        folder_path = '/Users/benjaminzubaly/Downloads/ready_for_preprocessing'\n        process_files(folder_path)\n        ```\n        :::\n\n\n[^1]: Python scripts were developed working with ChatGPT using the GPT-4 model [@openai].\n\n## Text Analysis Procedure\n\n1.  Once txt files are preprocessed and ready for text analysis, text analysis tools will be used to measure features of language.\n2.  For [Linguistic Inquiry and Word Count (LIWC)](https://www.liwc.app \"LIWC Tool Website\") [@boyd2022], [Automatic Readability Tool for English (ARTE)](https://nlp.gsu.edu/home \"ARTE Tool Website\"), and [The Lexical Suite](http://www.lexicalsuite.com \"The Lexical Suite Tool Website\") [@rocklage2023] papers will be analyzed by uploading the clean txt files directly and saving output as unique csv file.\n    -   These csv files will later be merged with the study data set using the following Python script that uses the DOI as the identifier.\n\n\n        ::: {.cell python.reticulate='false'}\n        \n        ```{.python .cell-code}\n        import os\n        import pandas as pd\n        \n        # To specify my user-specific information\n        username = 'benjaminzubaly'\n        base_path = f'/Users/{username}/Downloads'\n        \n        # To specify paths to the master file and the directory for the output file\n        master_file_path = os.path.join(base_path, 'thesis_study_data.csv')\n        output_dir = os.path.join(base_path, 'merged_data')\n        \n        # To check if the output directory exists\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # To load the master file as a data frame\n        master_df = pd.read_csv(master_file_path)\n        \n        # Specifying prefixes to add to columns in new merged_data (to easily identify which text analysis program the variable is coming from)\n        file_info = {\n            'ARTE_data.csv': 'ARTE_',\n            'lexical_suite_data.csv': 'LS_',\n            'LIWC_data.csv': 'LIWC_'\n        }\n        \n        # To merge each file into the master file with prefixed column names\n        for file_name, prefix in file_info.items():\n            file_path = os.path.join(base_path, file_name)\n            df = pd.read_csv(file_path)\n        \n            # To rename new columns with the specified prefix (without changing DOI column)\n            df.rename(columns={col: f'{prefix}{col}' for col in df.columns if col != 'DOI'}, inplace=True)\n        \n            # To merge with master dataframe based on DOI\n            master_df = pd.merge(master_df, df, on='DOI', how='left')\n        \n        # Save the merged file in the new 'merged_data' folder directory\n        output_file_path = os.path.join(output_dir, 'merged_thesis_study_data.csv')\n        master_df.to_csv(output_file_path, index=False)\n        ```\n        :::\n\n\n# Data Analysis\n\nThe following section will detail all processes of data analysis in a chronological order. All data analysis will be conducted in R [@r:alan2023] using the RStudio IDE [@rstudioteam2023], and all analyses will be ran and documented within a Quarto document [@quarto2023]. The data exploration section will roughly explain procedures that will be used to explore the data before further analyses. The hypothesis testing section will detail the procedures of testing the study hypotheses, including each null hypothesis significance test (NHST) and the justification for it. It will also include the R code for each analysis.[^2] For each NHST, an alpha of p \\< .05 will be used to determine statistical significance.\n\n[^2]: R code was developed working with the Data Analysis GPT tool by OpenAI [@openai2023].\n\nAlthough the following may not represent the actual variable names in the final study dataset, the following abbreviations will be used to refer to study variables in the code:\n\n`PaperID`: Unique identifier for each paper (i.e., the paper DOI).\n\n`PaperType`: Categorical variable indicating SAFP, SAGP, MAFP, or MAGP.\n\n`LingObf`: Continuous variable for linguistic obfuscation.\n\n`CertSent`: Continuous variable for certainty sentiment.\n\n`Refs`: Count variable for references.\n\n`FraudCorrAuth`: Dichotomous variable indicating if the fraudulent author is the corresponding author (1) or is not (0). Unknown cases will be marked in a seperate variable with this variable left blank.\n\n`NumAuth`: Count variable indicating the number of authors for each paper.\n\n`abstraction`: Abstraction index composed of the sum of standardized scores for `article`, `prep`, and `quantity`.\n\n`article`: Articles from LIWC.\n\n`prep`: Prepositions from LIWC.\n\n`quantity`: Quantities from LIWC.\n\n`cause`: Causation terms from LIWC.\n\n`jargon`: The percent of words not captured by LIWC (100-`Dic`).\n\n`Dic`: The percentage of words captured by all LIWC dictionaries.\n\n`emo_pos`: Positive emotion terms from LIWC.\n\n`flesch_re`: Flesch Reading Ease from ARTE.\n\n1.  Installing Necessary Packages:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    install.packages(\"readr\")       # For reading data\n    install.packages(\"dplyr\")       # For data manipulation and handling of missing data\n    install.packages(\"psych\")       # For descriptive statistics, correlations\n    install.packages(\"ggplot2\")     # For data visualization\n    install.packages(\"car\")         # For diagnostic tests such as Levene's test\n    install.packages(\"rcompanion\")  # For Games-Howell post-hoc test (if applicable)\n    install.packages(\"dunn.test\")   # For Dunn post-hoc test (if applicable)\n    ```\n    :::\n\n\n2.  Importing and Displaying Data:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    library(readr) # For loading package to read study dataset\n    \n    data <- read_csv(\"path_to_study_data.csv\") # For loading in study dataset\n    \n    data # For displaying study dataset\n    ```\n    :::\n\n\n## Data Cleaning\n\nTo conduct the analysis, we will first need to calculate the `LingObf` variable by calculating the `abstraction` index and `jargon` words; creating standardized scores for `abstraction`, `cause`, `jargon`, `emo_pos`, and `flesch_re`; and calculating the `LingObf` composite variable from these standardized scores.\n\n1.  First, we will calculate the `abstraction` index by creating standardized scores for `article`, `prep`, and `quantity` and summing them.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Calculate standardized scores for article, prep, and quantity and add them to the dataset\n    data$articles_standardized <- scale(data$articles)\n    data$prep_standardized <- scale(data$prep)\n    data$quantity_standardized <- scale(data$quantity)\n    \n    # Create the new variable 'abstraction' as the sum of the three standardized variables\n    data$abstraction <- data$articles_standardized + data$prep_standardized + data$quantity_standardized\n    \n    # Display the updated dataset\n    data\n    ```\n    :::\n\n\n2.  Next, we will calculate the `jargon` words by subtracting `Dic` from 100.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Calculate the new variable 'jargon' by subtracting 'Dic' from 100\n    data$jargon <- 100 - data$Dic\n    \n    # Display the updated dataset\n    data\n    ```\n    :::\n\n\n3.  Next, we will create standardized scores for each subcomponent of the `LingObf`.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Standardize the new set of variables and add them to the dataset\n    data$abstraction_standardized <- scale(data$abstraction)\n    data$cause_standardized <- scale(data$cause)\n    data$jargon_standardized <- scale(data$jargon)\n    data$emo_pos_standardized <- scale(data$emo_pos)\n    data$flesch_re_standardized <- scale(data$flesch_re)\n    \n    # Display the updated dataset\n    data\n    ```\n    :::\n\n\n4.  Finally, we will calculate the `LingObf` variable using the following formula: \\[cause_standardized + abstraction_standardized + jargon_standardized\\] -- \\[emo_pos_standardized + flesch_re_standardized\\].\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Calculate 'LingObf'\n    data$LingObf <- (data$cause_standardized + data$abstraction_standardized + data$jargon_standardized) - \n                    (data$emo_pos_standardized + data$flesch_re_standardized)\n    \n    # Display the updated dataset\n    data\n    ```\n    :::\n\n\n5.  For good measure, we will save this updated dataset as a csv file.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Specifying the new file path to downloads\n    clean_study_data_file_path <- \"/Users/benjaminzubaly/Downloads/thesis_study_dataset_clean.csv\"\n    \n    # Saving the clean dataset as a new CSV file\n    write_csv(data, clean_study_data_file_path)\n    ```\n    :::\n\n\n## Data Exploration\n\n### Dealing with Missing Data\n\n1.  Data will be first inspected for missing scores.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    library(dplyr) # Loading package for data manipulation and handling missing values\n    \n    # To summarize the number of missing values in each column\n    missing_data_summary <- sapply(data, function(x) sum(is.na(x)))\n    \n    print(missing_data_summary) # To see summary of missing values for all columns\n    ```\n    :::\n\n\n2.  For any missing categorical data, the cell will be left empty so as not to introduce bias.\n\n3.  For any missing continuous data, we will impute the mean value of the variable within `PaperType` using the following code. This will allow us to maintain the integrity of group means by not biasing the score from the influence of the scores for other paper groups.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Imputing missing values with the mean of the respective PaperType group\n    data <- data %>%\n      group_by(PaperType) %>%\n      mutate(\n        LingObf = ifelse(is.na(LingObf), mean(LingObf, na.rm = TRUE), LingObf),\n        CertSent = ifelse(is.na(CertSent), mean(CertSent, na.rm = TRUE), CertSent),\n        Refs = ifelse(is.na(Refs), mean(Refs, na.rm = TRUE), Refs)\n      ) %>%\n      ungroup()\n    ```\n    :::\n\n\n### Descriptive Statistics\n\n1.  General descriptive statistics will then be produced for each variable and for each variable within `PaperType` groups using the `psych` package.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    library(psych) # To load psych package\n    \n    # Generating descriptive statistics for the entire study dataset\n    descriptive_stats_all_data <- describe(data)\n    print(descriptive_stats_all_data)\n    \n    # Generating descriptive statistics within PaperType groups\n    descriptive_stats_by_PaperType <- describeby(data, group = data$PaperType)\n    print(descriptive_stats_by_PaperType)\n    ```\n    :::\n\n\n2.  Frequency tables will be produced for categorical variables, both for the data in general and within `PaperType` groups. A proportion table will be produced to more easily compare frequencies across `PaperType` groups.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To produce frequency table for all categorical variables\n    table(data$CategoricalVariable)\n    \n    # To produce frequency table for frequencies within PaperType groups for all categorical variables\n    frequency_table_by_PaperType <- table(data$PaperType, data$CategoricalVariable)\n    \n    # Proportions table to compare frequencies across PaperType groups\n    proportion_table_by_PaperType <- prop.table(frequency_table_by_PaperType, margin = 1)\n    \n    print(frequency_table_by_PaperType)\n    print(proportion_table_by_PaperType)\n    ```\n    :::\n\n\n### Data Visualization\n\n1.  Using the `ggplot2` package, frequency distributions and box plots will be generated for `LingObf`, `CertSent`, and `Refs`. A bar plot will be produced for `FraudCorrAuth`.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    library(ggplot2) # To load the ggplot2 package\n    \n    # To produce histogram for a continuous variable (IMPORTANT: \"ContinuousVariable\" should be replaced with the actual variable name)\n    ggplot(data, aes(x = ContinuousVariable)) + \n      geom_histogram(binwidth = 1, fill = \"blue\", color = \"black\") +\n      labs(title = \"Histogram of ContinuousVariable for Entire Dataset\")\n    \n    # To produce box plots for a continuous variable (IMPORTANT: \"ContinuousVariable\" should be replaced with the actual variable name)\n    ggplot(data, aes(y = ContinuousVariable)) + \n      geom_boxplot(fill = \"lightblue\", color = \"black\") +\n      labs(title = \"Box Plot of ContinuousVariable for Entire Dataset\")\n    \n    # To produce bar plot for FraudCorrAuth\n    ggplot(data, aes(x = FraudCorrAuth)) + \n      geom_bar(fill = \"coral\") +\n      labs(title = \"Bar Plot of FraudCorrAuth for Entire Dataset\")\n    ```\n    :::\n\n\n2.  Using the `ggplot2` package, frequency distributions and box plots will be generated for `LingObf`, `CertSent`, and `Refs` within `PaperType` groups. Bar plots will be produced for `FraudCorrAuth` within `PaperType` groups.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To produce histograms for a continuous variable within PaperType groups (IMPORTANT: \"ContinuousVariable\" should be replaced with the actual variable name)\n    ggplot(data, aes(x = ContinuousVariable)) + \n      geom_histogram(binwidth = 1, fill = \"blue\", color = \"black\") +\n      facet_wrap(~ PaperType) +\n      labs(title = \"Histograms of ContinuousVariable by PaperType\")\n    \n    # To produce box plots for a continuous variable within PaperType groups (IMPORTANT: \"ContinuousVariable\" should be replaced with the actual variable name)\n    ggplot(data, aes(x = PaperType, y = ContinuousVariable)) + \n      geom_boxplot(fill = \"lightblue\", color = \"black\") +\n      labs(title = \"Box Plot of ContinuousVariable by PaperType\")\n    \n    # To produce bar plots for FraudCorrAuth within PaperType groups\n    ggplot(data, aes(x = PaperType, fill = FraudCorrAuth)) + \n      geom_bar(position = \"dodge\") +\n      labs(title = \"Bar Plot of FraudCorrAuth by PaperType\")\n    ```\n    :::\n\n\n### Bivariate Correlations\n\nIn order to further explore data characteristics, bivariate correlations between outcome variables will be produced. As these are only for exploring data and not testing predictions, assumptions will not be tested, and the relationships will not be probed for statistical significance. Then, correlations for subcomponents of composite variables will be produced.\n\n1.  Pearson bivariate correlations will be produced for `LingObf`, `CertSent`, and `Refs`.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To produce correlation matrix for LingObf, CertSent, and Refs variables within whole dataset\n    \n    # To calculate the Pearson correlation coefficients between the numerical outcome variables\n    correlation_matrix_num_outcomes <- cor(data[c(\"LingObf\", \"CertSent\", \"Refs\")], use = \"complete.obs\", method = \"pearson\")\n    \n    # To print the correlation matrix for numerical outcome variables\n    print(correlation_matrix_num_outcomes)\n    ```\n    :::\n\n\n2.  Point-biserial correlations will be produced between `FraudCorrAuth` and `LingObf`, `CertSent`, and `Refs`.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Calculating Point-biserial correlations between FraudCorrAuth and numerical outcome variables\n    pbcorr_FraudCorrAuth_LingObf <- cor(data$FraudCorrAuth, data$LingObf, use = \"complete.obs\", method = \"pearson\")\n    pbcorr_FraudCorrAuth_CertSent <- cor(data$FraudCorrAuth, data$CertSent, use = \"complete.obs\", method = \"pearson\")\n    pbcorr_FraudCorrAuth_Refs <- cor(data$FraudCorrAuth, data$Refs, use = \"complete.obs\", method = \"pearson\")\n    \n    # Printing the correlation coefficients\n    print(pbcorr_FraudCorrAuth_LingObf)\n    print(pbcorr_FraudCorrAuth_CertSent)\n    print(pbcorr_FraudCorrAuth_Refs)\n    ```\n    :::\n\n\n3.  Pearson bivariate correlations will be produced for the subcomponents of the `abstraction` index.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Calculate pairwise correlations among the subcomponents of the abstraction index\n    correlation_matrix_abstraction_subcomponents <- cor(data[,c(\"articles\", \"prep\", \"quantity\")])\n    \n    # Display the correlation matrix\n    correlation_matrix_abstraction_subcomponents\n    ```\n    :::\n\n\n4.  Pearson bivariate correlations will be produced for the subcomponents of the `LingObf` index.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # Calculate pairwise correlations among the subcomponents of LingObf\n    correlation_matrix_LingObf_subcomponents <- cor(data[,c(\"cause\", \"abstraction\", \"jargon\", \"emo_pos\", \"flesch_re\")])\n    \n    # Display the correlation matrix\n    correlation_matrix_LingObf_subcomponents\n    ```\n    :::\n\n\n## Testing Hypotheses\n\nWe will test the following hypotheses:\n\n### **Hypothesis 1: Linguistic Obfuscation Hypothesis**\n\n**Hypothesis 1** consists of two variants. The first, **Hypothesis 1a**, is the general linguistic obfuscation hypothesis tested by @markowitz2016 which we will test in order to replicate their previous findings. The second, **Hypothesis 1b**, consists of our specific variant of the linguistic obfuscation hypothesis that states fraudulent scientists obfuscate their writing to make their work less accessible to their research group, rather than those outside their research group. This leads to the prediction that SAFP---which presumably are written without the presence of a research group---will use less linguistic obfuscation. Specifically, these hypotheses are stated follows:\n\n**Hypothesis 1a:** Fraudulent research will be written with more linguistic obfuscation than non-fraudulent research. \\[Replication\\]\n\n**Hypothesis 1b:** Single-author fraudulent research will be written with more linguistic obfuscation than non-fraudulent research but less linguistic obfuscation than multi-author fraudulent research. \\[Novel\\]\n\n#### Testing Hypothesis 1a {#head-test_hyp_one_a_head}\n\nTo test **Hypothesis 1a**, we will conduct an independent samples t-test comparing the mean levels of `LingObf` of fraudulent papers (SAFP and MAFP combined) with genuine papers (SAGP and MAGP combined). That is, we will see whether the mean linguistic obfuscation differs between fraudulent papers and genuine papers.\n\n1.  To compare fraudulent publications with genuine publications, we will first create a new data frame that contains a dichotomous grouping variable for genuine or fraudulent papers (`Genuine_or_Fraudulent`). That is, the new data frame will combine SAGP and MAGP into one category (`GPaper`) and SAFP and MAFP into another category (`FPaper`). This will be done using the `dplyr` package.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To create the new data frame with new dichotomous variable Genuine_or_Fraudulent\n    fraud_or_not_df <- df %>%\n      mutate(Genuine_or_Fraudulent = ifelse(PaperType %in% c(\"SAFP\", \"MAFP\"), \"FPaper\", \"GPaper\"))\n    \n    # To double-check that the new data frame was created properly\n    head(fraud_or_not_df)\n    ```\n    :::\n\n\n2.  To check the assumption of normality we will use the `ggplot2` package to create Q-Q plots for `LingObf` within the `FPaper` and `GPaper` groups. The Q-Q plots will be investigated visually. Because each group will have approximately n=60 cases, our t-test should be robust to violations of this assumption. However, if the assumption of normality is grossly violated we will use a bootstrapped t-test to fit a more robust model.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To produce Q-Q plot for LingObf within the FPaper and GPaper groups\n    qplot(sample = fraud_or_not_df$LingObf, geom = \"qq\") +\n      facet_wrap(~Genuine_or_Fraudulent)\n    ```\n    :::\n\n\n3.  To check the assumption of homogeneity of variances, we will conduct Levene's test using the `car` package.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    library(car) # To load the car package\n    \n    # To conduct Levene's test for homogeneity of variances\n    hyp_one_a_levene_test <- leveneTest(LingObf ~ Genuine_or_Fraudulent, data = fraud_or_not_df)\n    \n    # To print Levene's test result\n    print(hyp_one_a_levene_test)\n    ```\n    :::\n\n\n4.  Once assumptions are tested, we will run the independent samples t-test using the new data frame to determine whether there is a difference between fraudulent papers and genuine papers. If Levene's test rejects the assumption of homogeneity of variance (i.e., if the variances of the two groups are significantly different with an ⍺ = .05), then we will run Welch's t-test instead.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To conduct the regular independent samples t-test\n    hyp_one_a_t_test <- t.test(LingObf ~ Genuine_or_Fraudulent, data = fraud_or_not_df)\n    \n    # To print the regular independent samples t-test result\n    print(hyp_one_a_t_test)\n    \n    # To conduct Welch's t-test if Levene's test is significant\n    hyp_one_a_welch_t_test <- t.test(LingObf ~ Genuine_or_Fraudulent, data = fraud_or_not_df, var.equal = FALSE)\n    \n    # To print the Welch's t-test result\n    print(hyp_one_a_welch_t_test)\n    ```\n    :::\n\n\n#### Testing Hypothesis 1b\n\nTo test **Hypothesis 1b** we will conduct a one-way analysis of variance (ANOVA) to compare group means of `LingObf` for SAFP, MAFP, SAGP, and MAGP. That is, we will determine whether SAFP contains more linguistic obfuscation than the genuine paper groups but less linguistic obfuscation than the MAFP.\n\n1.  To check the assumption of normality we will use the `ggplot2` package to create Q-Q plots for `LingObf` using our original study dataset. The Q-Q plots will be investigated visually. If the assumption of normality is violated we will use a bootstrapped t-test to fit a more robust model.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To produce Q-Q plot for LingObf within the PaperType groups\n    qplot(sample = data$LingObf, geom = \"qq\") +\n      facet_wrap(~PaperType)\n    ```\n    :::\n\n\n2.  To test the assumption of homogeneity of variances, we will conduct Levene's test.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To conduct Levene's test for homogeneity of variances\n    hyp_one_b_levene_test <- leveneTest(LingObf ~ PaperType, data = data)\n    \n    # To print Levene's test results\n    print(hyp_one_b_levene_test)\n    ```\n    :::\n\n\n3.  Once assumptions are tested, we will run the ANOVA using the original study data to determine whether there is a difference exists between the four groups. If Levene's test rejects the assumption of homogeneity of variance (i.e., if the variances of the four groups are significantly different with an ⍺ = .05), then we will run Welch's ANOVA instead.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To conduct the one-way ANOVA if assumptions are met\n    hyp_one_b_anova <- aov(LingObf ~ PaperType, data = data)\n    \n    # To see summary of one-way ANOVA results\n    summary(hyp_one_b_anova)\n    \n    # To conduct the Welch's ANOVA if assumption of homogeneity of variance is violated\n    hyp_one_b_welch_anova <- oneway.test(LingObf ~ PaperType, data = data, var.equal = FALSE)\n    \n    # To see summary of one-way Welch's ANOVA results\n    summary(hyp_one_b_welch_anova)\n    ```\n    :::\n\n\n4.  If a significant difference is found between the `PaperType` groups based on the one-way ANOVA, a post-hoc test will be used to determine where the differences lie. Tukey's Highly Significant Difference (HSD) test will be used if a regular one-way ANOVA was used, which will adjust p-values to avoid inflating the family-wise error rate. If the Welch's ANOVA was used, we will follow it up with the Games-Howell post-hoc test instead using the `rcompanion` package.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To conduct Tukey's HSD test if ANOVA indicates significant difference\n    hyp_one_b_TukeyHSD <- TukeyHSD(hyp_one_b_anova)\n    \n    # To print Tukey's HSD test results\n    print(hyp_one_b_TukeyHSD)\n    \n    # To conduct Games-Howell test if Welch's ANOVA was used\n    library(rcompanion) # To load rcompanion for Games-Howell test\n    \n    # To run Games-Howell post-hoc test\n    hyp_one_b_games_howell <- pairwiseGamesHowellTest(hyp_one_b_welch_anova)\n    \n    # To print Games-Howell test results\n    print(hyp_one_b_games_howell)\n    ```\n    :::\n\n\n### **Hypothesis 2:** References Hypothesis\n\n**Hypothesis 2** also consists of two variants. The first is that fraudulent research will contain more references than non-fraudulent research, functioning to make the research more costly to assess from outside readers [@markowitz2016b] or as an analogue to third-person pronoun usage in other linguistic studies of deception [@schmidt2022]. The second is our adaptation of the first version which emphasizes the salience of the research group as the audience of this communicative style, similar to the logic of Hypothesis 1b. Specifically, the hypotheses are as follows:\n\n**Hypothesis 2a:** Fraudulent research will contain more references than non-fraudulent research. \\[Replication\\]\n\n**Hypothesis 2b:** Single-author fraudulent research will contain more references than non-fraudulent research but fewer references than multi-author fraudulent research. \\[Novel\\]\n\n#### Testing Hypothesis 2a\n\nTo test **Hypothesis 2a**, we will use the previously created data frame `fraud_or_not_df`[^3] to compare the mean number of `Refs` between the `FPaper` and `GPaper` groups using an independent-samples t-test.\n\n[^3]: See [Testing Hypothesis 1a](#head-test_hyp_one_a_head) for details regarding the creation of this data frame.\n\n1.  To check the assumption of normality we will use the `ggplot2` package to create Q-Q plots for `Refs` within our `fraud_or_not_df` data frame within the `Genuine_or_Fraudulent` dichotomous variable. The Q-Q plots will be investigated visually. As references are technically count, rather than continuous, data, there is a higher likelihood that it will violate the assumption of normality, so in this case we will instead run a Mann-Whitney U test to compare the two groups by their median `Refs`.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To produce Q-Q plots for Refs within the FPaper and GPaper groups\n    qplot(sample = fraud_or_not_df$Refs, geom = \"qq\") +\n      facet_wrap(~Genuine_or_Fraudulent)\n    \n    # If the assumption of normality is supported, move to Step 2 to test the assumption of homogeneity of variance.\n    \n    # If the assumption of normality is violated, move to Step 3 to conduct a Mann-Whitney U test.\n    ```\n    :::\n\n\n2.  To test the assumption of homogeneity of variances, we will conduct Levene's test.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To conduct Levene's test for homogeneity of variances\n    hyp_two_a_levene_test <- leveneTest(Refs ~ Genuine_or_Fraudulent, data = fraud_or_not_df)\n    \n    # To print Levene's test result\n    print(hyp_two_a_levene_test)\n    ```\n    :::\n\n\n3.  Once assumptions are tested, we will run the independent samples t-test using the `fraud_or_not_df` data frame to determine whether there is a difference between fraudulent papers and genuine papers. If Levene's test rejected the assumption of homogeneity of variance (i.e., if the variances of the two groups are significantly different with an ⍺ = .05), then we will run a Mann-Whitney U test instead, which could be better for our count data than the parametric t-test.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To conduct the regular independent samples t-test\n    hyp_two_a_t_test <- t.test(Refs ~ Genuine_or_Fraudulent, data = fraud_or_not_df)\n    \n    # To print the regular independent samples t-test result\n    print(hyp_two_a_t_test)\n    \n    # To conduct Mann-Whitney U test if assumptions are not supported\n    hyp_two_a_mann_test <- wilcox.test(Refs ~ Genuine_or_Fraudulent, data = fraud_or_not_df)\n    \n    # To print the Mann-Whitney U test result\n    print(hyp_two_a_mann_test)\n    ```\n    :::\n\n\n#### Testing Hypothesis 2b\n\nTo test **Hypothesis 2b** we will conduct a one-way analysis of variance (ANOVA) to compare group means of `Refs` for SAFP, MAFP, SAGP, and MAGP. That is, we will determine whether SAFP contains more linguistic obfuscation than the genuine paper groups but less linguistic obfuscation than the MAFP. As references are technically count, rather than continuous, data, there is a higher likelihood that it will violate the assumption of normality, so in this case we will instead run a Mann-Whitney U test to compare the two groups by their median `Refs`.\n\n1.  To check the assumption of normality we will use the `ggplot2` package to create Q-Q plots for `Refs` using our original study dataset. The Q-Q plots will be investigated visually. If the assumption of normality is violated we will use the non-parametric Kruskal-Wallis test instead.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To produce Q-Q plot for Refs within the PaperType groups\n    qplot(sample = data$Refs, geom = \"qq\") +\n      facet_wrap(~PaperType)\n    \n    # If the assumption of normality is supported, move to Step 2 to test the assumption of homogeneity of variance.\n    \n    # If the assumption of normality is violated, move to Step 3 to conduct a Kruskal-Wallis Test test.\n    ```\n    :::\n\n\n2.  To test the assumption of homogeneity of variances, we will conduct Levene's test.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To conduct Levene's test for homogeneity of variances\n    hyp_two_b_levene_test <- leveneTest(Refs ~ PaperType, data = data)\n    \n    # To print Levene's test results\n    print(hyp_two_b_levene_test)\n    ```\n    :::\n\n\n3.  Once assumptions are tested, we will run the ANOVA using the original study data to determine whether there is a difference exists between the four groups. If Levene's test rejects the assumption of homogeneity of variance (i.e., if the variances of the four groups are significantly different with an ⍺ = .05), then we will run the Kruskal-Wallis test instead instead.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To conduct the one-way ANOVA if assumptions are met\n    hyp_two_b_anova <- aov(Refs ~ PaperType, data = data)\n    \n    # To see summary of one-way ANOVA results\n    summary(hyp_two_b_anova)\n    \n    # To conduct Kruskal-Wallis test if assumptions are violated\n    hyp_two_b_krusk <- kruskal.test(Refs ~ PaperType, data = data)\n    \n    # To print the results of the Kruskal-Wallis test\n    print(hyp_two_b_krusk)\n    ```\n    :::\n\n\n4.  If a significant difference is found between the `PaperType` groups based on the one-way ANOVA, a post-hoc test will be used to determine where the differences lie. Tukey's Highly Significant Difference (HSD) test will be used if a regular one-way ANOVA was used, which will adjust p-values to avoid inflating the family-wise error rate. If the Kruskal-Wallis test was used, we will follow it up with the Dunn post-hoc test instead using the `dunn.test` package and a Bonferroni correction to reduce family-wise error rates.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To conduct Tukey's HSD test if ANOVA indicates significant difference\n    hyp_two_b_TukeyHSD <- TukeyHSD(hyp_one_b_anova)\n    \n    # To print Tukey's HSD test results\n    print(hyp_two_b_TukeyHSD)\n    \n    # To conduct Dunn post-hoc test if used Kruskal-Wallis test\n    library(dunn.test) # To load the dunn.test package\n    \n    # To run Dunn's post-hoc test with bonferonni correction\n    hyp_two_b_dunn <- dunn.test(data$Refs, data$PaperType, method=\"bonferroni\")\n    \n    # To print results of Dunn's post-hoc test\n    print(hyp_two_b_dunn)\n    ```\n    :::\n\n\n### **Hypothesis 3:** Certainty\n\n**Hypothesis 3** investigates the use of certainty language in cases of scientific fraud. While a case study of Deidrik Stapel tended to use more certain language [@markowitz2014], others have found less certainty language in retracted papers than non-retracted papers [@dehdarirad2023]. Thus, **Hypothesis 3** is meant to provide clarity regarding the use of certainty language in scientific fraud. Specifically, it is stated as follows:\n\n**Hypothesis 3:** Fraudulent research will contain less certainty than non-fraudulent research. \\[Replication\\]\n\n#### Testing Hypothesis 3:\n\nTo test **Hypothesis 3**, we will use the previously created data frame `fraud_or_not_df`[^4] to compare the mean `CertSent` score between the `FPaper` and `GPaper` groups using an independent-samples t-test.\n\n[^4]: See [Testing Hypothesis 1a](#head-test_hyp_one_a_head) for details regarding the creation of this data frame.\n\n1.  To check the assumption of normality we will use the `ggplot2` package to create Q-Q plots for `CertSent` within the `FPaper` and `GPaper` groups. The Q-Q plots will be investigated visually. Because each group will have approximately n=60 cases, our t-test should be robust to violations of this assumption. However, if the assumption of normality is grossly violated we will use a bootstrapped t-test to fit a more robust model.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To produce Q-Q plot for CertSent within the FPaper and GPaper groups\n    qplot(sample = fraud_or_not_df$CertSent, geom = \"qq\") +\n      facet_wrap(~Genuine_or_Fraudulent)\n    ```\n    :::\n\n\n2.  To check the assumption of homogeneity of variances, we will conduct Levene's test using the `car` package.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To conduct Levene's test for homogeneity of variances\n    hyp_three_levene_test <- leveneTest(CertSent ~ Genuine_or_Fraudulent, data = fraud_or_not_df)\n    \n    # To print Levene's test result\n    print(hyp_three_levene_test)\n    ```\n    :::\n\n\n3.  Once assumptions are tested, we will run the independent samples t-test using the `fraud_or_not_df` data frame to determine whether there is a difference between fraudulent papers and genuine papers. If Levene's test rejects the assumption of homogeneity of variance (i.e., if the variances of the two groups are significantly different with an ⍺ = .05), then we will run Welch's t-test instead.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To conduct the regular independent samples t-test\n    hyp_three_t_test <- t.test(CertSent ~ Genuine_or_Fraudulent, data = fraud_or_not_df)\n    \n    # To print the regular independent samples t-test result\n    print(hyp_three_t_test)\n    \n    # To conduct Welch's t-test if Levene's test is significant\n    hyp_three_welch_t_test <- t.test(CertSent ~ Genuine_or_Fraudulent, data = fraud_or_not_df, var.equal = FALSE)\n    \n    # To print the Welch's t-test result\n    print(hyp_three_welch_t_test)\n    ```\n    :::\n\n\n### **Hypothesis 4:**\n\n**Hypothesis 4** is a logical extension of the notion that to hide information you must first have control over related information flow. Based on this principle, **Hypothesis 4** is formulated as follows:\n\n**Hypothesis 4:** For multi-author fraudulent papers, the fraudulent author will be the corresponding author more frequently than other authors. \\[Novel\\]\n\n**Hypothesis 4** tests on the following two assumptions:\n\n-   *First, all authors on a paper are equally likely to be the corresponding author.* Although this assumption oversimplifies norms of assigning scientists to be corresponding authors, in the absence of more information regarding, for example, author order, author responsibilities, or laboratory status, it is the most accurate prediction we can make regarding the likelihood of an author being the corresponding author. That is, in the absence of other information it is the baseline prediction.\n-   *Second, if fraudulent authors to not attempt to control information flow, they are equally likely to be the corresponding as their coauthors are.* This may also oversimplify the norms of assigning scientists to be corresponding authors. For example, it is possible that fraudulent authors perform more data management, and it is possible that scientists that are responsible for data management are more likely to be corresponding authors. However, given the absence of this information (e.g., regarding research group norms), this assumption is reasonable.\n\nThese two assumptions allow us to make a prediction regarding the baseline expected frequency that fraudulent authors will be the corresponding authors of fraudulent research papers if they do not attempt to control information flow.\n\n#### Testing Hypothesis 4:\n\nTo test **Hypothesis 4** we will conduct a binomial test to compare the observes likelihood that the fraudulent author is the corresponding author (i.e., `FraudCorrAuth`) to the expected probability given the assumption of equal likelihood for all authors and the average number of authors on each MAFP.\n\n1.  To attain the expected probabiltiy we will calculate the inverse of the mean number of authors on all MAFP where the corresponding author is known. To do this, we will first need to filter out other `PaperType` categories and MAFP cases where the corresponding author is unknown (i.e., `FraudCorrAuth` has a missing value) to make a new data frame, `hypothesis_four_df`. Then, we will calculate the mean `NumAuth` within this data frame and take its inverse.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To create data frame that only includes MAFP with a score for FraudCorrAuth\n    hypotheeis_four_df <- data[data$PaperType == 'MAFP' & !is.na(data$FraudCorrAuth)]\n    \n    # To print the data frame to check it\n    print(hypotheeis_four_df)\n    \n    # To calculate the mean number of authors within the data frame\n    mean_MAFP_NumAuth_corrauth_known <- mean(hypotheeis_four_df$num_authors)\n    \n    # To display the mean value\n    print(mean_MAFP_NumAuth_corrauth_known)\n    \n    # To calculate the inverse of mean_MAFP_NumAuth_corrauth_known\n    expected_prob_FraudCorrAuth <- 1 / mean_MAFP_NumAuth_corrauth_known\n    \n    # To display the expected probability\n    print(expected_prob_FraudCorrAuth)\n    ```\n    :::\n\n\n2.  Next, we will conduct a binomial test to compare the observed likelihood that a corresponding author is the fraudulent author to the expected probability (`expected_prob_FraudCorrAuth`).\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    # To run the binomial test with the expected probability within the hypothesis four data frame\n    hyp_four_binomial_test <- binom.test(sum(hypotheeis_four_df$FraudCorrAuth), nrow(data), p = expected_prob_FraudCorrAuth, alternative = \"two.sided\")\n    \n    # To print the binomial test results\n    print(hyp_four_binomial_test)\n    ```\n    :::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}