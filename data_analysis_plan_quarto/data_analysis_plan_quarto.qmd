---
title: "Psychology of Scientific Fraud Data Analysis Plan"
author: Benjamin Zubaly
date: November 19, 2023
format: 
  html:
    self-contained: true
    toc: true
knitr:
  opts_chunk: 
      warning: false
      message: false
editor: visual
bibliography: references.bib
csl: apa.csl
---

```{R setup}
knitr::opts_chunk$set(eval = FALSE)
```

# Preparation for Data Analysis

The following subsections will detail the processes of (1) data collection, (2) data entry, (3) data preprocessing, and (4) text analysis procedure as relatively linearly to allow this document to guide a linear workflow during execution of the project. Some procedures may not fit neatly within the three subsections neatly while maintaining this workflow-oriented documentation, and some procedures may not be named in their exact procedural chronology because they are more usefully categorized within these three subsections.

## Data Collection

1.  Single-author fraudulent papers (SAFP) will be identified from the Retraction Watch database by sorting by the author and "reason" column.
    -   Fraudulent papers will be defined as any paper marked with a "data fabrication" or "data falsification" label in the reason column.
    -   A new csv file (hereafter "study dataset") will be created with these papers only and all of the data from the Retraction Watch dataset.
2.  Multi-author fraudulent papers (MAFP) will then be identified from the retraction watch database and matched according to our criteria (see methodology doc) with SAFPs by sorting the retraction watch database along journal and year. Individual papers will then be investigated through internet searches to attempt matching by other criteria.
    -   All data from the Retraction Watch database will be retained when adding MAFP to study dataset.
3.  Once all fraudulent papers have been assembled, online searches for matched papers will be conducted to attain papers with matched characteristics according to our criteria (see methodology doc).
    -   Genuine papers will be matched with fraudulent papers within author number groups (e.g., single fraudulent matched with single genuine).
        -   This will ensure matching across all groups where between-group statistical comparisons will be conducted.
4.  During the collection of all paper types, PDFs of the papers will be saved into separate files on the primary author's computer and identified with their DOI.
    -   References will be counted from these original PDFs and entered into the study data. This will avoid miscounts due to counting references after potential alterations to references are made during data preprocessing procedures.
    -   Number of authors will also be counted from these original PDFs and entered into the study data. This will avoid miscounts due to counting authors after potential alterations to this text are made during data preprocessing procedures.

## Data Entry

1.  As mentioned above, data from the retraction watch dataset will be retained for our study data. This data will be done for all fraudulent papers and whenever possible for genuine papers as well.
2.  During the search for and collection of all paper types, data regarding the matching criteria will be recorded and entered by hand.
3.  During the search for and collection of all multi-author fraudulent papers, data regarding whether the fraudulent author is the corresponding author will be collected and entered into the study data as a dichotomous variable.
    -   The ground truth for whether an author is the fraudulent author will be determined by investigating retraction watch article linked in the retraction watch database.
        -   When making judgments as to attributions of fraud in the articles, we will make the preliminary judgment and study data entry, and he include the parts of the articles that he believes make this decision justified. If there is not enough information to make a judgment, the case will be marked as unknown.
        -   Then, a second judge will examine these sentences, and in any case of uncertainty the original article will be discussed by the two judges.
        -   If there remains a dispute, the case will be noted as unknown.

## **Data Preprocessing:**

Data preprocessing procedures will utilize Python [@python2023]. All Python scripts will be executed in [Jupyter Notebook](https://jupyter.org "Link to Jupyter Website Home Page") [@jupyter2023].

-   All PDFs will be automatically converted to txt file formats using the following Python script[^1] (based on `fitz`).

    ```{python, python.reticulate=FALSE}
    import fitz  # PyMuPDF
    import os

    # To define the directory containing the PDFs for text extraction
    pdfs_dir = '/Users/benjaminzubaly/Downloads/PDFs4Extraction'

    # To define the directory for the output text files
    output_dir = '/Users/benjaminzubaly/Downloads/pdftotxt'

    # To create the output directory if it doesn't exist
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # To loop through each file in the PDFs directory
    for filename in os.listdir(pdfs_dir):
        # To check if the file is a PDF
        if filename.lower().endswith('.pdf'):  # Lower() is used to handle case-insensitive extensions
            # Full path to the current PDF file
            pdf_path = os.path.join(pdfs_dir, filename)

            # To extract the base name of the PDF file (without the extension)
            base_name, _ = os.path.splitext(os.path.basename(pdf_path))

            # To name the new txt file with 'plain_text_' at begininning
            txt_file_name = os.path.join(output_dir, f'plain_text_{base_name}.txt')

            # To open the PDF file
            doc = fitz.open(pdf_path)

            # To open the txt file for writing
            with open(txt_file_name, 'w') as txt_file:
                # To iterate through each page and extract text
                for page_num in range(len(doc)):
                    page = doc.load_page(page_num)  # Load the page
                    text = page.get_text()  # Extract text from the page
                    txt_file.write(text)  # Write the text to the txt file

            # To close the document
            doc.close()
    ```

-   These text files will then each be opened manually to remove abstracts, tables, figures, etc. Only main body text will be left in the text files.

    -   Full texts of abstracts will be retained and entered into the study data set via copy and paste.

-   The text will then be cleaned using the following Python script.

    -   The script removes periods that do not end sentences; converts all characters to lowercase; removes all forms of the words retract, withdrawn, introduction, method, procedure, and discussion; removes all relatively common special characters in scientific writing; removes footnotes; removes numbers; replaces newline characters with a space; and removes dates.

    -   This is necessary for reliable text analysis metrics, particularly readability statistics, which heavily rely on words per sentence (and require the removal of special characters such as non-sentence-ending periods).

        ```{python, python.reticulate=FALSE}
        import os
        import re

        def clean_text(text):
            # List of common abbreviations with periods
            abbreviations = [
                "Dr\.", "Mr\.", "Mrs\.", "Ms\.", "Jr\.", "Sr\.", "Co\.", "Inc\.", "Ltd\.", "Corp\.", "Prof\.",
                "Asst\.", "Assoc\.", "Capt\.", "Sgt\.", "Lt\.", "Gen\.", "Pvt\.", "Cpl\.", "Maj\.", "Gov\.",
                "Pres\.", "V\.P\.", "Sec\.", "Treas\.", "Mgr\.", "Dir\.", "Dep\.", "Rep\.", "Sen\.", "Amb\.",
                "Ch\.", "Art\.", "Sect\.", "Est\.", "Dept\.", "Ave\.", "Blvd\.", "St\.", "Rd\.", "Ln\.",
                "Ct\.", "Sq\.", "Hts\.", "Mtn\.", "Pkwy\.", "Cir\.", "Pl\.", "Bldg\.", "Fl\.", "Apt\.",
                "Ste\.", "P\.O\.", "U\.S\.", "U\.K\.", "E\.U\.", "U\.N\."
            ]
            abbreviations_pattern = r'\b(?:' + '|'.join(abbreviations) + r')'
            text = re.sub(abbreviations_pattern, lambda x: x.group().replace('.', ''), text, flags=re.IGNORECASE)

            # Remove periods not at the end of sentences or document
            text = re.sub(r'\.(?=(?!\s[A-Z])|$)', '', text)
            # Convert to lowercase
            text = text.lower()
            # Remove 'retract' and its derivatives in all forms
            text = re.sub(r'\bretract(ing|ion|s|ed)?\b', '', text, flags=re.IGNORECASE)
            # Remove forms of "withdraw" or "withdrawn"
            text = re.sub(r'\bwithdraw(n|s|ing)?\b', '', text, flags=re.IGNORECASE)
            # Remove a wide range of special characters, Greek letters, degree symbol, and superscript numbers 0-10
            special_chars = r'[@#$%^&*\(\)_\-+=\{\}\[\]\|\\/:;"\'<>,~€£¥¢©®™±²³µπ÷×√∞∑∆≈≠≤≥∂∫∝∩∪∈∉∅∀∃∴∵∼≡↔αβγδεζηθικλμνξοπρστυφχψωΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩ∇⊥°₀₁₂₃₄₅₆₇₈₉₁₀]'
            text = re.sub(special_chars, '', text)
            # Replace newline characters with space
            text = text.replace('\n', ' ')
            # Remove all numbers (even within or directly next to letters)
            text = re.sub(r'\d+', '', text)
            # Remove common date formats (basic pattern)
            text = re.sub(r'\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\s+\d{1,2},\s+\d{4}\b', '', text)
            # Remove specified words in all forms
            text = re.sub(r'\b(introduction|methods|procedure|discussion)s?\b', '', text, flags=re.IGNORECASE)
            # Additional cleaning rules can be added here
            return text

        def process_files(folder_path):
            # Create a new folder for cleaned files
            clean_folder_path = '/Users/benjaminzubaly/Downloads/clean_txt'
            if not os.path.exists(clean_folder_path):
                os.makedirs(clean_folder_path)

            for filename in os.listdir(folder_path):
                if filename.endswith('.txt'):
                    file_path = os.path.join(folder_path, filename)
                    with open(file_path, 'r') as file:
                        content = file.read()

                    cleaned_content = clean_text(content)

                    # Rename the file with 'clean_' prefix and save in the new folder
                    new_filename = 'clean_' + filename.replace('prepared_', '')
                    new_file_path = os.path.join(clean_folder_path, new_filename)

                    with open(new_file_path, 'w') as file:
                        file.write(cleaned_content)

        # Path to the folder
        folder_path = '/Users/benjaminzubaly/Downloads/ready_for_preprocessing'
        process_files(folder_path)
        ```

[^1]: Python scripts were developed working with ChatGPT using the GPT-4 model [@openai].

## Text Analysis Procedure

1.  Once txt files are preprocessed and ready for text analysis, text analysis tools will be used to measure features of language.

2.  For [Linguistic Inquiry and Word Count (LIWC)](https://www.liwc.app "LIWC Tool Website") [@boyd2022], [The Lexical Suite](http://www.lexicalsuite.com "The Lexical Suite Tool Website") [@rocklage2023], and our custom Python script that utilizes the `textstat` module [@aggarwal2022] to calculate Flesch Reading Ease [@flesch1948], papers will be analyzed by inputting the clean txt files directly and saving output as unique csv file.

    -   Script for measuring Flesch Reading Ease and creating a CSV file.

        ```{python, python.reticulate=FALSE}
        import glob
        import textstat
        import csv

        # Path to the folder containing text files
        text_files_path = '/Users/benjaminzubaly/Downloads/clean_txt/*.txt'

        # Path for the output CSV file
        output_csv_path = '/Users/benjaminzubaly/Downloads/flesch_scores.csv'

        with open(output_csv_path, 'w', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['DOI', 'flesch_re'])

            for path in glob.glob(text_files_path):
                with open(path, 'r') as file:
                    text = file.read()
                    score = textstat.flesch_reading_ease(text)
                    # Extract filename from path, remove prefix and extension
                    filename = path.split('/')[-1].replace('clean_', '').replace('.txt', '').replace(':', '/')
                    writer.writerow([filename, score])
        ```

3.  These csv files will later be merged with the study data set using the following Python script that uses the DOI as the identifier.

    ```{python, python.reticulate=FALSE}
    import os
    import pandas as pd

    # To specify my user-specific information
    username = 'benjaminzubaly'
    base_path = f'/Users/{username}/Downloads'

    # To specify paths to the master file and the directory for the output file
    master_file_path = os.path.join(base_path, 'thesis_study_data.csv')
    output_dir = os.path.join(base_path, 'merged_data')

    # To check if the output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # To load the master file as a data frame
    master_df = pd.read_csv(master_file_path)

    # Specifying prefixes to add to columns in new merged_data (to easily identify which text analysis program the variable is coming from)
    file_info = {
        'ARTE_data.csv': 'ARTE_',
        'lexical_suite_data.csv': 'LS_',
        'LIWC_data.csv': 'LIWC_'
    }

    # To merge each file into the master file with prefixed column names
    for file_name, prefix in file_info.items():
        file_path = os.path.join(base_path, file_name)
        df = pd.read_csv(file_path)

        # To rename new columns with the specified prefix (without changing DOI column)
        df.rename(columns={col: f'{prefix}{col}' for col in df.columns if col != 'DOI'}, inplace=True)

        # To merge with master dataframe based on DOI
        master_df = pd.merge(master_df, df, on='DOI', how='left')

    # Save the merged file in the new 'merged_data' folder directory
    output_file_path = os.path.join(output_dir, 'merged_thesis_study_data.csv')
    master_df.to_csv(output_file_path, index=False)
    ```

# Data Analysis

The following section will detail all processes of data analysis in a chronological order. All data analysis will be conducted in R [@r:alan2023] using the RStudio IDE [@rstudioteam2023], and all analyses will be ran and documented within a Quarto document [@quarto2023]. The data exploration section will roughly explain procedures that will be used to explore the data before further analyses. The hypothesis testing section will detail the procedures of testing the study hypotheses, including each null hypothesis significance test (NHST) and the justification for it. It will also include the R code for each analysis.[^2] For each NHST, an alpha of p \< .05 will be used to determine statistical significance.

[^2]: R code was developed working with the Data Analysis GPT tool by OpenAI [@openai2023].

Although the following may not represent the actual variable names in the final study dataset, the following abbreviations will be used to refer to study variables in the code:

`PaperID`: Unique identifier for each paper (i.e., the paper DOI).

`PaperType`: Categorical variable indicating SAFP, SAGP, MAFP, or MAGP.

`LingObf`: Continuous variable for linguistic obfuscation.

`CertSent`: Continuous variable for certainty sentiment.

`Refs`: Count variable for references.

`FraudCorrAuth`: Dichotomous variable indicating if the fraudulent author is the corresponding author (1) or is not (0). Unknown cases will be marked in a seperate variable with this variable left blank.

`NumAuth`: Count variable indicating the number of authors for each paper.

`abstraction`: Abstraction index composed of the sum of standardized scores for `article`, `prep`, and `quantity`.

`article`: Articles from LIWC.

`prep`: Prepositions from LIWC.

`quantity`: Quantities from LIWC.

`cause`: Causation terms from LIWC.

`jargon`: The percent of words not captured by LIWC (100-`Dic`).

`Dic`: The percentage of words captured by all LIWC dictionaries.

`emo_pos`: Positive emotion terms from LIWC.

`flesch_re`: Flesch Reading Ease from ARTE.

1.  Installing Necessary Packages:

    ```{R}
    install.packages("readr")       # For reading data
    install.packages("dplyr")       # For data manipulation and handling of missing data
    install.packages("psych")       # For descriptive statistics, correlations
    install.packages("ggplot2")     # For data visualization
    install.packages("car")         # For diagnostic tests such as Levene's test
    install.packages("rcompanion")  # For Games-Howell post-hoc test (if applicable)
    install.packages("dunn.test")   # For Dunn post-hoc test (if applicable)
    ```

2.  Importing and Displaying Data:

    ```{R}
    library(readr) # For loading package to read study dataset

    data <- read_csv("path_to_study_data.csv") # For loading in study dataset

    data # For displaying study dataset
    ```

## Data Cleaning

To conduct the analysis, we will first need to calculate the `LingObf` variable by calculating the `abstraction` index and `jargon` words; creating standardized scores for `abstraction`, `cause`, `jargon`, `emo_pos`, and `flesch_re`; and calculating the `LingObf` composite variable from these standardized scores.

1.  First, we will calculate the `abstraction` index by creating standardized scores for `article`, `prep`, and `quantity` and summing them.

    ```{R}
    # Calculate standardized scores for article, prep, and quantity and add them to the dataset
    data$articles_standardized <- scale(data$articles)
    data$prep_standardized <- scale(data$prep)
    data$quantity_standardized <- scale(data$quantity)

    # Create the new variable 'abstraction' as the sum of the three standardized variables
    data$abstraction <- data$articles_standardized + data$prep_standardized + data$quantity_standardized

    # Display the updated dataset
    data
    ```

2.  Next, we will calculate the `jargon` words by subtracting `Dic` from 100.

    ```{R}
    # Calculate the new variable 'jargon' by subtracting 'Dic' from 100
    data$jargon <- 100 - data$Dic

    # Display the updated dataset
    data
    ```

3.  Next, we will create standardized scores for each subcomponent of the `LingObf`.

    ```{R}
    # Standardize the new set of variables and add them to the dataset
    data$abstraction_standardized <- scale(data$abstraction)
    data$cause_standardized <- scale(data$cause)
    data$jargon_standardized <- scale(data$jargon)
    data$emo_pos_standardized <- scale(data$emo_pos)
    data$flesch_re_standardized <- scale(data$flesch_re)

    # Display the updated dataset
    data
    ```

4.  Finally, we will calculate the `LingObf` variable using the following formula: \[cause_standardized + abstraction_standardized + jargon_standardized\] -- \[emo_pos_standardized + flesch_re_standardized\].

    ```{R}
    # Calculate 'LingObf'
    data$LingObf <- (data$cause_standardized + data$abstraction_standardized + data$jargon_standardized) - 
                    (data$emo_pos_standardized + data$flesch_re_standardized)

    # Display the updated dataset
    data
    ```

5.  For good measure, we will save this updated dataset as a csv file.

    ```{R}
    # Specifying the new file path to downloads
    clean_study_data_file_path <- "/Users/benjaminzubaly/Downloads/thesis_study_dataset_clean.csv"

    # Saving the clean dataset as a new CSV file
    write_csv(data, clean_study_data_file_path)
    ```

## Data Exploration

### Dealing with Missing Data

1.  Data will be first inspected for missing scores.

    ```{R}
    library(dplyr) # Loading package for data manipulation and handling missing values

    # To summarize the number of missing values in each column
    missing_data_summary <- sapply(data, function(x) sum(is.na(x)))

    print(missing_data_summary) # To see summary of missing values for all columns
    ```

2.  For any missing categorical data, the cell will be left empty so as not to introduce bias.

3.  For any missing continuous data, we will impute the mean value of the variable within `PaperType` using the following code. This will allow us to maintain the integrity of group means by not biasing the score from the influence of the scores for other paper groups.

    ```{R}
    # Imputing missing values with the mean of the respective PaperType group
    data <- data %>%
      group_by(PaperType) %>%
      mutate(
        LingObf = ifelse(is.na(LingObf), mean(LingObf, na.rm = TRUE), LingObf),
        CertSent = ifelse(is.na(CertSent), mean(CertSent, na.rm = TRUE), CertSent),
        Refs = ifelse(is.na(Refs), mean(Refs, na.rm = TRUE), Refs)
      ) %>%
      ungroup()
    ```

### Descriptive Statistics

1.  General descriptive statistics will then be produced for each variable and for each variable within `PaperType` groups using the `psych` package.

    ```{R}
    library(psych) # To load psych package

    # Generating descriptive statistics for the entire study dataset
    descriptive_stats_all_data <- describe(data)
    print(descriptive_stats_all_data)

    # Generating descriptive statistics within PaperType groups
    descriptive_stats_by_PaperType <- describeby(data, group = data$PaperType)
    print(descriptive_stats_by_PaperType)
    ```

2.  Frequency tables will be produced for categorical variables, both for the data in general and within `PaperType` groups. A proportion table will be produced to more easily compare frequencies across `PaperType` groups.

    ```{R}
    # To produce frequency table for all categorical variables
    table(data$CategoricalVariable)

    # To produce frequency table for frequencies within PaperType groups for all categorical variables
    frequency_table_by_PaperType <- table(data$PaperType, data$CategoricalVariable)

    # Proportions table to compare frequencies across PaperType groups
    proportion_table_by_PaperType <- prop.table(frequency_table_by_PaperType, margin = 1)

    print(frequency_table_by_PaperType)
    print(proportion_table_by_PaperType)
    ```

### Data Visualization

1.  Using the `ggplot2` package, frequency distributions and box plots will be generated for `LingObf`, `CertSent`, and `Refs`. A bar plot will be produced for `FraudCorrAuth`.

    ```{R}
    library(ggplot2) # To load the ggplot2 package

    # To produce histogram for a continuous variable (IMPORTANT: "ContinuousVariable" should be replaced with the actual variable name)
    ggplot(data, aes(x = ContinuousVariable)) + 
      geom_histogram(binwidth = 1, fill = "blue", color = "black") +
      labs(title = "Histogram of ContinuousVariable for Entire Dataset")

    # To produce box plots for a continuous variable (IMPORTANT: "ContinuousVariable" should be replaced with the actual variable name)
    ggplot(data, aes(y = ContinuousVariable)) + 
      geom_boxplot(fill = "lightblue", color = "black") +
      labs(title = "Box Plot of ContinuousVariable for Entire Dataset")

    # To produce bar plot for FraudCorrAuth
    ggplot(data, aes(x = FraudCorrAuth)) + 
      geom_bar(fill = "coral") +
      labs(title = "Bar Plot of FraudCorrAuth for Entire Dataset")
    ```

2.  Using the `ggplot2` package, frequency distributions and box plots will be generated for `LingObf`, `CertSent`, and `Refs` within `PaperType` groups. Bar plots will be produced for `FraudCorrAuth` within `PaperType` groups.

    ```{R}
    # To produce histograms for a continuous variable within PaperType groups (IMPORTANT: "ContinuousVariable" should be replaced with the actual variable name)
    ggplot(data, aes(x = ContinuousVariable)) + 
      geom_histogram(binwidth = 1, fill = "blue", color = "black") +
      facet_wrap(~ PaperType) +
      labs(title = "Histograms of ContinuousVariable by PaperType")

    # To produce box plots for a continuous variable within PaperType groups (IMPORTANT: "ContinuousVariable" should be replaced with the actual variable name)
    ggplot(data, aes(x = PaperType, y = ContinuousVariable)) + 
      geom_boxplot(fill = "lightblue", color = "black") +
      labs(title = "Box Plot of ContinuousVariable by PaperType")

    # To produce bar plots for FraudCorrAuth within PaperType groups
    ggplot(data, aes(x = PaperType, fill = FraudCorrAuth)) + 
      geom_bar(position = "dodge") +
      labs(title = "Bar Plot of FraudCorrAuth by PaperType")
    ```

### Bivariate Correlations

In order to further explore data characteristics, bivariate correlations between outcome variables will be produced. As these are only for exploring data and not testing predictions, assumptions will not be tested, and the relationships will not be probed for statistical significance. Then, correlations for subcomponents of composite variables will be produced.

1.  Pearson bivariate correlations will be produced for `LingObf`, `CertSent`, and `Refs`.

    ```{R}
    # To produce correlation matrix for LingObf, CertSent, and Refs variables within whole dataset

    # To calculate the Pearson correlation coefficients between the numerical outcome variables
    correlation_matrix_num_outcomes <- cor(data[c("LingObf", "CertSent", "Refs")], use = "complete.obs", method = "pearson")

    # To print the correlation matrix for numerical outcome variables
    print(correlation_matrix_num_outcomes)
    ```

2.  Point-biserial correlations will be produced between `FraudCorrAuth` and `LingObf`, `CertSent`, and `Refs`.

    ```{R}
    # Calculating Point-biserial correlations between FraudCorrAuth and numerical outcome variables
    pbcorr_FraudCorrAuth_LingObf <- cor(data$FraudCorrAuth, data$LingObf, use = "complete.obs", method = "pearson")
    pbcorr_FraudCorrAuth_CertSent <- cor(data$FraudCorrAuth, data$CertSent, use = "complete.obs", method = "pearson")
    pbcorr_FraudCorrAuth_Refs <- cor(data$FraudCorrAuth, data$Refs, use = "complete.obs", method = "pearson")

    # Printing the correlation coefficients
    print(pbcorr_FraudCorrAuth_LingObf)
    print(pbcorr_FraudCorrAuth_CertSent)
    print(pbcorr_FraudCorrAuth_Refs)
    ```

3.  Pearson bivariate correlations will be produced for the subcomponents of the `abstraction` index.

    ```{R}
    # Calculate pairwise correlations among the subcomponents of the abstraction index
    correlation_matrix_abstraction_subcomponents <- cor(data[,c("articles", "prep", "quantity")])

    # Display the correlation matrix
    correlation_matrix_abstraction_subcomponents
    ```

4.  Pearson bivariate correlations will be produced for the subcomponents of the `LingObf` index.

    ```{R}
    # Calculate pairwise correlations among the subcomponents of LingObf
    correlation_matrix_LingObf_subcomponents <- cor(data[,c("cause", "abstraction", "jargon", "emo_pos", "flesch_re")])

    # Display the correlation matrix
    correlation_matrix_LingObf_subcomponents
    ```

## Testing Hypotheses

We will test the following hypotheses:

### **Hypothesis 1: Linguistic Obfuscation Hypothesis**

**Hypothesis 1** consists of two variants. The first, **Hypothesis 1a**, is the general linguistic obfuscation hypothesis tested by @markowitz2016 which we will test in order to replicate their previous findings. The second, **Hypothesis 1b**, consists of our specific variant of the linguistic obfuscation hypothesis that states fraudulent scientists obfuscate their writing to make their work less accessible to their research group, rather than those outside their research group. This leads to the prediction that SAFP---which presumably are written without the presence of a research group---will use less linguistic obfuscation. Specifically, these hypotheses are stated follows:

**Hypothesis 1a:** Fraudulent research will be written with more linguistic obfuscation than non-fraudulent research. \[Replication\]

**Hypothesis 1b:** Single-author fraudulent research will be written with more linguistic obfuscation than non-fraudulent research but less linguistic obfuscation than multi-author fraudulent research. \[Novel\]

#### Testing Hypothesis 1a {#head-test_hyp_one_a_head}

To test **Hypothesis 1a**, we will conduct an independent samples t-test comparing the mean levels of `LingObf` of fraudulent papers (SAFP and MAFP combined) with genuine papers (SAGP and MAGP combined). That is, we will see whether the mean linguistic obfuscation differs between fraudulent papers and genuine papers.

1.  To compare fraudulent publications with genuine publications, we will first create a new data frame that contains a dichotomous grouping variable for genuine or fraudulent papers (`Genuine_or_Fraudulent`). That is, the new data frame will combine SAGP and MAGP into one category (`GPaper`) and SAFP and MAFP into another category (`FPaper`). This will be done using the `dplyr` package.

    ```{R}
    # To create the new data frame with new dichotomous variable Genuine_or_Fraudulent
    fraud_or_not_df <- df %>%
      mutate(Genuine_or_Fraudulent = ifelse(PaperType %in% c("SAFP", "MAFP"), "FPaper", "GPaper"))

    # To double-check that the new data frame was created properly
    head(fraud_or_not_df)
    ```

2.  To check the assumption of normality we will use the `ggplot2` package to create Q-Q plots for `LingObf` within the `FPaper` and `GPaper` groups. The Q-Q plots will be investigated visually. Because each group will have approximately n=60 cases, our t-test should be robust to violations of this assumption. However, if the assumption of normality is grossly violated we will use a bootstrapped t-test to fit a more robust model.

    ```{R}
    # To produce Q-Q plot for LingObf within the FPaper and GPaper groups
    qplot(sample = fraud_or_not_df$LingObf, geom = "qq") +
      facet_wrap(~Genuine_or_Fraudulent)
    ```

3.  To check the assumption of homogeneity of variances, we will conduct Levene's test using the `car` package.

    ```{R}
    library(car) # To load the car package

    # To conduct Levene's test for homogeneity of variances
    hyp_one_a_levene_test <- leveneTest(LingObf ~ Genuine_or_Fraudulent, data = fraud_or_not_df)

    # To print Levene's test result
    print(hyp_one_a_levene_test)
    ```

4.  Once assumptions are tested, we will run the independent samples t-test using the new data frame to determine whether there is a difference between fraudulent papers and genuine papers. If Levene's test rejects the assumption of homogeneity of variance (i.e., if the variances of the two groups are significantly different with an ⍺ = .05), then we will run Welch's t-test instead.

    ```{R}
    # To conduct the regular independent samples t-test
    hyp_one_a_t_test <- t.test(LingObf ~ Genuine_or_Fraudulent, data = fraud_or_not_df)

    # To print the regular independent samples t-test result
    print(hyp_one_a_t_test)

    # To conduct Welch's t-test if Levene's test is significant
    hyp_one_a_welch_t_test <- t.test(LingObf ~ Genuine_or_Fraudulent, data = fraud_or_not_df, var.equal = FALSE)

    # To print the Welch's t-test result
    print(hyp_one_a_welch_t_test)
    ```

#### Testing Hypothesis 1b

To test **Hypothesis 1b** we will conduct a one-way analysis of variance (ANOVA) to compare group means of `LingObf` for SAFP, MAFP, SAGP, and MAGP. That is, we will determine whether SAFP contains more linguistic obfuscation than the genuine paper groups but less linguistic obfuscation than the MAFP.

1.  To check the assumption of normality we will use the `ggplot2` package to create Q-Q plots for `LingObf` using our original study dataset. The Q-Q plots will be investigated visually. If the assumption of normality is violated we will use a bootstrapped t-test to fit a more robust model.

    ```{R}
    # To produce Q-Q plot for LingObf within the PaperType groups
    qplot(sample = data$LingObf, geom = "qq") +
      facet_wrap(~PaperType)
    ```

2.  To test the assumption of homogeneity of variances, we will conduct Levene's test.

    ```{R}
    # To conduct Levene's test for homogeneity of variances
    hyp_one_b_levene_test <- leveneTest(LingObf ~ PaperType, data = data)

    # To print Levene's test results
    print(hyp_one_b_levene_test)
    ```

3.  Once assumptions are tested, we will run the ANOVA using the original study data to determine whether there is a difference exists between the four groups. If Levene's test rejects the assumption of homogeneity of variance (i.e., if the variances of the four groups are significantly different with an ⍺ = .05), then we will run Welch's ANOVA instead.

    ```{R}
    # To conduct the one-way ANOVA if assumptions are met
    hyp_one_b_anova <- aov(LingObf ~ PaperType, data = data)

    # To see summary of one-way ANOVA results
    summary(hyp_one_b_anova)

    # To conduct the Welch's ANOVA if assumption of homogeneity of variance is violated
    hyp_one_b_welch_anova <- oneway.test(LingObf ~ PaperType, data = data, var.equal = FALSE)

    # To see summary of one-way Welch's ANOVA results
    summary(hyp_one_b_welch_anova)
    ```

4.  If a significant difference is found between the `PaperType` groups based on the one-way ANOVA, a post-hoc test will be used to determine where the differences lie. Tukey's Highly Significant Difference (HSD) test will be used if a regular one-way ANOVA was used, which will adjust p-values to avoid inflating the family-wise error rate. If the Welch's ANOVA was used, we will follow it up with the Games-Howell post-hoc test instead using the `rcompanion` package.

    ```{R}
    # To conduct Tukey's HSD test if ANOVA indicates significant difference
    hyp_one_b_TukeyHSD <- TukeyHSD(hyp_one_b_anova)

    # To print Tukey's HSD test results
    print(hyp_one_b_TukeyHSD)

    # To conduct Games-Howell test if Welch's ANOVA was used
    library(rcompanion) # To load rcompanion for Games-Howell test

    # To run Games-Howell post-hoc test
    hyp_one_b_games_howell <- pairwiseGamesHowellTest(hyp_one_b_welch_anova)

    # To print Games-Howell test results
    print(hyp_one_b_games_howell)
    ```

### **Hypothesis 2:** References Hypothesis

**Hypothesis 2** also consists of two variants. The first is that fraudulent research will contain more references than non-fraudulent research, functioning to make the research more costly to assess from outside readers [@markowitz2016b] or as an analogue to third-person pronoun usage in other linguistic studies of deception [@schmidt2022]. The second is our adaptation of the first version which emphasizes the salience of the research group as the audience of this communicative style, similar to the logic of Hypothesis 1b. Specifically, the hypotheses are as follows:

**Hypothesis 2a:** Fraudulent research will contain more references than non-fraudulent research. \[Replication\]

**Hypothesis 2b:** Single-author fraudulent research will contain more references than non-fraudulent research but fewer references than multi-author fraudulent research. \[Novel\]

#### Testing Hypothesis 2a

To test **Hypothesis 2a**, we will use the previously created data frame `fraud_or_not_df`[^3] to compare the mean number of `Refs` between the `FPaper` and `GPaper` groups using an independent-samples t-test.

[^3]: See [Testing Hypothesis 1a](#head-test_hyp_one_a_head) for details regarding the creation of this data frame.

1.  To check the assumption of normality we will use the `ggplot2` package to create Q-Q plots for `Refs` within our `fraud_or_not_df` data frame within the `Genuine_or_Fraudulent` dichotomous variable. The Q-Q plots will be investigated visually. As references are technically count, rather than continuous, data, there is a higher likelihood that it will violate the assumption of normality, so in this case we will instead run a Mann-Whitney U test to compare the two groups by their median `Refs`.

    ```{R}
    # To produce Q-Q plots for Refs within the FPaper and GPaper groups
    qplot(sample = fraud_or_not_df$Refs, geom = "qq") +
      facet_wrap(~Genuine_or_Fraudulent)

    # If the assumption of normality is supported, move to Step 2 to test the assumption of homogeneity of variance.

    # If the assumption of normality is violated, move to Step 3 to conduct a Mann-Whitney U test.
    ```

2.  To test the assumption of homogeneity of variances, we will conduct Levene's test.

    ```{R}
    # To conduct Levene's test for homogeneity of variances
    hyp_two_a_levene_test <- leveneTest(Refs ~ Genuine_or_Fraudulent, data = fraud_or_not_df)

    # To print Levene's test result
    print(hyp_two_a_levene_test)
    ```

3.  Once assumptions are tested, we will run the independent samples t-test using the `fraud_or_not_df` data frame to determine whether there is a difference between fraudulent papers and genuine papers. If Levene's test rejected the assumption of homogeneity of variance (i.e., if the variances of the two groups are significantly different with an ⍺ = .05), then we will run a Mann-Whitney U test instead, which could be better for our count data than the parametric t-test.

    ```{R}
    # To conduct the regular independent samples t-test
    hyp_two_a_t_test <- t.test(Refs ~ Genuine_or_Fraudulent, data = fraud_or_not_df)

    # To print the regular independent samples t-test result
    print(hyp_two_a_t_test)

    # To conduct Mann-Whitney U test if assumptions are not supported
    hyp_two_a_mann_test <- wilcox.test(Refs ~ Genuine_or_Fraudulent, data = fraud_or_not_df)

    # To print the Mann-Whitney U test result
    print(hyp_two_a_mann_test)
    ```

#### Testing Hypothesis 2b

To test **Hypothesis 2b** we will conduct a one-way analysis of variance (ANOVA) to compare group means of `Refs` for SAFP, MAFP, SAGP, and MAGP. That is, we will determine whether SAFP contains more linguistic obfuscation than the genuine paper groups but less linguistic obfuscation than the MAFP. As references are technically count, rather than continuous, data, there is a higher likelihood that it will violate the assumption of normality, so in this case we will instead run a Mann-Whitney U test to compare the two groups by their median `Refs`.

1.  To check the assumption of normality we will use the `ggplot2` package to create Q-Q plots for `Refs` using our original study dataset. The Q-Q plots will be investigated visually. If the assumption of normality is violated we will use the non-parametric Kruskal-Wallis test instead.

    ```{R}
    # To produce Q-Q plot for Refs within the PaperType groups
    qplot(sample = data$Refs, geom = "qq") +
      facet_wrap(~PaperType)

    # If the assumption of normality is supported, move to Step 2 to test the assumption of homogeneity of variance.

    # If the assumption of normality is violated, move to Step 3 to conduct a Kruskal-Wallis Test test.
    ```

2.  To test the assumption of homogeneity of variances, we will conduct Levene's test.

    ```{R}
    # To conduct Levene's test for homogeneity of variances
    hyp_two_b_levene_test <- leveneTest(Refs ~ PaperType, data = data)

    # To print Levene's test results
    print(hyp_two_b_levene_test)
    ```

3.  Once assumptions are tested, we will run the ANOVA using the original study data to determine whether there is a difference exists between the four groups. If Levene's test rejects the assumption of homogeneity of variance (i.e., if the variances of the four groups are significantly different with an ⍺ = .05), then we will run the Kruskal-Wallis test instead instead.

    ```{R}
    # To conduct the one-way ANOVA if assumptions are met
    hyp_two_b_anova <- aov(Refs ~ PaperType, data = data)

    # To see summary of one-way ANOVA results
    summary(hyp_two_b_anova)

    # To conduct Kruskal-Wallis test if assumptions are violated
    hyp_two_b_krusk <- kruskal.test(Refs ~ PaperType, data = data)

    # To print the results of the Kruskal-Wallis test
    print(hyp_two_b_krusk)
    ```

4.  If a significant difference is found between the `PaperType` groups based on the one-way ANOVA, a post-hoc test will be used to determine where the differences lie. Tukey's Highly Significant Difference (HSD) test will be used if a regular one-way ANOVA was used, which will adjust p-values to avoid inflating the family-wise error rate. If the Kruskal-Wallis test was used, we will follow it up with the Dunn post-hoc test instead using the `dunn.test` package and a Bonferroni correction to reduce family-wise error rates.

    ```{R}
    # To conduct Tukey's HSD test if ANOVA indicates significant difference
    hyp_two_b_TukeyHSD <- TukeyHSD(hyp_one_b_anova)

    # To print Tukey's HSD test results
    print(hyp_two_b_TukeyHSD)

    # To conduct Dunn post-hoc test if used Kruskal-Wallis test
    library(dunn.test) # To load the dunn.test package

    # To run Dunn's post-hoc test with bonferonni correction
    hyp_two_b_dunn <- dunn.test(data$Refs, data$PaperType, method="bonferroni")

    # To print results of Dunn's post-hoc test
    print(hyp_two_b_dunn)
    ```

### **Hypothesis 3:** Certainty

**Hypothesis 3** investigates the use of certainty language in cases of scientific fraud. While a case study of Deidrik Stapel tended to use more certain language [@markowitz2014], others have found less certainty language in retracted papers than non-retracted papers [@dehdarirad2023]. Thus, **Hypothesis 3** is meant to provide clarity regarding the use of certainty language in scientific fraud. Specifically, it is stated as follows:

**Hypothesis 3:** Fraudulent research will contain less certainty than non-fraudulent research. \[Replication\]

#### Testing Hypothesis 3:

To test **Hypothesis 3**, we will use the previously created data frame `fraud_or_not_df`[^4] to compare the mean `CertSent` score between the `FPaper` and `GPaper` groups using an independent-samples t-test.

[^4]: See [Testing Hypothesis 1a](#head-test_hyp_one_a_head) for details regarding the creation of this data frame.

1.  To check the assumption of normality we will use the `ggplot2` package to create Q-Q plots for `CertSent` within the `FPaper` and `GPaper` groups. The Q-Q plots will be investigated visually. Because each group will have approximately n=60 cases, our t-test should be robust to violations of this assumption. However, if the assumption of normality is grossly violated we will use a bootstrapped t-test to fit a more robust model.

    ```{R}
    # To produce Q-Q plot for CertSent within the FPaper and GPaper groups
    qplot(sample = fraud_or_not_df$CertSent, geom = "qq") +
      facet_wrap(~Genuine_or_Fraudulent)
    ```

2.  To check the assumption of homogeneity of variances, we will conduct Levene's test using the `car` package.

    ```{R}
    # To conduct Levene's test for homogeneity of variances
    hyp_three_levene_test <- leveneTest(CertSent ~ Genuine_or_Fraudulent, data = fraud_or_not_df)

    # To print Levene's test result
    print(hyp_three_levene_test)
    ```

3.  Once assumptions are tested, we will run the independent samples t-test using the `fraud_or_not_df` data frame to determine whether there is a difference between fraudulent papers and genuine papers. If Levene's test rejects the assumption of homogeneity of variance (i.e., if the variances of the two groups are significantly different with an ⍺ = .05), then we will run Welch's t-test instead.

    ```{R}
    # To conduct the regular independent samples t-test
    hyp_three_t_test <- t.test(CertSent ~ Genuine_or_Fraudulent, data = fraud_or_not_df)

    # To print the regular independent samples t-test result
    print(hyp_three_t_test)

    # To conduct Welch's t-test if Levene's test is significant
    hyp_three_welch_t_test <- t.test(CertSent ~ Genuine_or_Fraudulent, data = fraud_or_not_df, var.equal = FALSE)

    # To print the Welch's t-test result
    print(hyp_three_welch_t_test)
    ```

### **Hypothesis 4:**

**Hypothesis 4** is a logical extension of the notion that to hide information you must first have control over related information flow. Based on this principle, **Hypothesis 4** is formulated as follows:

**Hypothesis 4:** For multi-author fraudulent papers, the fraudulent author will be the corresponding author more frequently than other authors. \[Novel\]

**Hypothesis 4** tests on the following two assumptions:

-   *First, all authors on a paper are equally likely to be the corresponding author.* Although this assumption oversimplifies norms of assigning scientists to be corresponding authors, in the absence of more information regarding, for example, author order, author responsibilities, or laboratory status, it is the most accurate prediction we can make regarding the likelihood of an author being the corresponding author. That is, in the absence of other information it is the baseline prediction.
-   *Second, if fraudulent authors to not attempt to control information flow, they are equally likely to be the corresponding as their coauthors are.* This may also oversimplify the norms of assigning scientists to be corresponding authors. For example, it is possible that fraudulent authors perform more data management, and it is possible that scientists that are responsible for data management are more likely to be corresponding authors. However, given the absence of this information (e.g., regarding research group norms), this assumption is reasonable.

These two assumptions allow us to make a prediction regarding the baseline expected frequency that fraudulent authors will be the corresponding authors of fraudulent research papers if they do not attempt to control information flow.

#### Testing Hypothesis 4:

To test **Hypothesis 4** we will conduct a binomial test to compare the observes likelihood that the fraudulent author is the corresponding author (i.e., `FraudCorrAuth`) to the expected probability given the assumption of equal likelihood for all authors and the average number of authors on each MAFP.

1.  To attain the expected probabiltiy we will calculate the inverse of the mean number of authors on all MAFP where the corresponding author is known. To do this, we will first need to filter out other `PaperType` categories and MAFP cases where the corresponding author is unknown (i.e., `FraudCorrAuth` has a missing value) to make a new data frame, `hypothesis_four_df`. Then, we will calculate the mean `NumAuth` within this data frame and take its inverse.

    ```{R}
    # To create data frame that only includes MAFP with a score for FraudCorrAuth
    hypotheeis_four_df <- data[data$PaperType == 'MAFP' & !is.na(data$FraudCorrAuth)]

    # To print the data frame to check it
    print(hypotheeis_four_df)

    # To calculate the mean number of authors within the data frame
    mean_MAFP_NumAuth_corrauth_known <- mean(hypotheeis_four_df$num_authors)

    # To display the mean value
    print(mean_MAFP_NumAuth_corrauth_known)

    # To calculate the inverse of mean_MAFP_NumAuth_corrauth_known
    expected_prob_FraudCorrAuth <- 1 / mean_MAFP_NumAuth_corrauth_known

    # To display the expected probability
    print(expected_prob_FraudCorrAuth)
    ```

2.  Next, we will conduct a binomial test to compare the observed likelihood that a corresponding author is the fraudulent author to the expected probability (`expected_prob_FraudCorrAuth`).

    ```{R}
    # To run the binomial test with the expected probability within the hypothesis four data frame
    hyp_four_binomial_test <- binom.test(sum(hypotheeis_four_df$FraudCorrAuth), nrow(data), p = expected_prob_FraudCorrAuth, alternative = "two.sided")

    # To print the binomial test results
    print(hyp_four_binomial_test)
    ```
